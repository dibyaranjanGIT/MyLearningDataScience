{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different scaling technique**\n",
    "\n",
    "## sklearn.preprocessing.MinMaxScaler¶\n",
    "\n",
    "Transform features by scaling each feature to a given range.\n",
    "This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "The transformation is given by:\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min\n",
    "\n",
    "\n",
    "fit(X[, y]) : Compute the minimum and maximum to be used for later scaling.\n",
    "fit_transform(X[, y]) : Fit to data, then transform it.\n",
    "\n",
    "\n",
    "## sklearn.preprocessing.RobustScaler¶\n",
    "\n",
    "This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n",
    "Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method.\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. \n",
    "In such cases, the median and the interquartile range often give better results.\n",
    "\n",
    "fit(X[, y]) : Compute the median and quantiles to be used for scaling.\n",
    "fit_transform(X[, y]) : Fit to data, then transform it.\n",
    "\n",
    "## sklearn.preprocessing.StandardScaler¶\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "z = (x - u) / s\n",
    "where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.\n",
    "\n",
    "\n",
    "fit(X[, y]) : Compute the mean and std to be used for later scaling.\n",
    "fit_transform(X[, y]) : Fit to data, then transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.reshape(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = np.linspace(1,50,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = arr2.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  3.57894737],\n",
       "       [ 6.15789474,  8.73684211],\n",
       "       [11.31578947, 13.89473684],\n",
       "       [16.47368421, 19.05263158],\n",
       "       [21.63157895, 24.21052632],\n",
       "       [26.78947368, 29.36842105],\n",
       "       [31.94736842, 34.52631579],\n",
       "       [37.10526316, 39.68421053],\n",
       "       [42.26315789, 44.84210526],\n",
       "       [47.42105263, 50.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "print(std_scaler.fit(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(std_scaler.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(std_scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.5666989  -1.5666989 ]\n",
      " [-1.21854359 -1.21854359]\n",
      " [-0.87038828 -0.87038828]\n",
      " [-0.52223297 -0.52223297]\n",
      " [-0.17407766 -0.17407766]\n",
      " [ 0.17407766  0.17407766]\n",
      " [ 0.52223297  0.52223297]\n",
      " [ 0.87038828  0.87038828]\n",
      " [ 1.21854359  1.21854359]\n",
      " [ 1.5666989   1.5666989 ]]\n"
     ]
    }
   ],
   "source": [
    "print(std_scaler.fit_transform(arr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "print(mm_scaler.fit(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(mm_scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.11111111 0.11111111]\n",
      " [0.22222222 0.22222222]\n",
      " [0.33333333 0.33333333]\n",
      " [0.44444444 0.44444444]\n",
      " [0.55555556 0.55555556]\n",
      " [0.66666667 0.66666667]\n",
      " [0.77777778 0.77777778]\n",
      " [0.88888889 0.88888889]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(mm_scaler.fit_transform(arr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rb_scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler()\n"
     ]
    }
   ],
   "source": [
    "print(Rb_scaler.fit(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5 -0.5]\n",
      " [-0.5 -0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "print(Rb_scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearn",
   "language": "python",
   "name": "machinelearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
